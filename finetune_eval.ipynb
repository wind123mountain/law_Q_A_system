{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu124'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BGE-M3**\n",
    "\n",
    "**Note:**\n",
    "\n",
    "**[BGE-M3 Fine-tune Guide](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder)**\n",
    "\n",
    "**[BGE-M3-Reranker-v2 Fine-tune Guide](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fine-tune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger('BGE_M3_FINE_TUNE')\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/09/2025 19:02:59 - INFO - numexpr.utils -   NumExpr defaulting to 8 threads.\n",
      "04/09/2025 19:03:02 - INFO - datasets -   PyTorch version 2.6.0+cu124 available.\n",
      "04/09/2025 19:03:02 - INFO - datasets -   TensorFlow version 2.10.1 available.\n"
     ]
    }
   ],
   "source": [
    "from BGE_M3.arguments import DataArguments, ModelArguments, RetrieverTrainingArguments\n",
    "from BGE_M3.data import CustomTrainDataset, CustomEmbedCollator\n",
    "from BGE_M3.modeling import BGEM3Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo đối tượng ModelArguments với tham số\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"BAAI/bge-m3\",\n",
    "    tokenizer_name=\"BAAI/bge-m3\",\n",
    "    cache_dir=None\n",
    ")\n",
    "\n",
    "# Khởi tạo đối tượng DataArguments với tham số\n",
    "data_args = DataArguments(\n",
    "    knowledge_distillation=True,\n",
    "    train_data=[\"./Data/train.jsonl\"],\n",
    "    cache_path=\"./data_cache\",\n",
    "    train_group_size=4,\n",
    "    query_max_len=256,\n",
    "    passage_max_len=400,\n",
    "    max_example_num_per_dataset=100000,\n",
    "    same_task_within_batch=True,\n",
    "    shuffle_ratio=0.0,\n",
    "    small_threshold=0,\n",
    "    drop_threshold=0,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Khởi tạo đối tượng RetrieverTrainingArguments với tham số\n",
    "training_args = RetrieverTrainingArguments(\n",
    "    output_dir=\"./bge-m3-output\",\n",
    "    negatives_cross_device=False,\n",
    "    temperature=0.02,\n",
    "    fix_position_embedding=True,\n",
    "    sentence_pooling_method='cls',\n",
    "    normlized=True,\n",
    "    enable_sub_batch=False,\n",
    "    unified_finetuning=False,\n",
    "    use_self_distill=False,\n",
    "    fix_encoder=False,\n",
    "    colbert_dim=-1,\n",
    "    self_distill_start_step=0,\n",
    "    per_device_train_batch_size=2,\n",
    "    sub_batch_size=-1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9fa0c14b564127921a86c5ee4fa9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/09/2025 19:03:52 - INFO - BGE_M3.modeling -   loading model-model_name: C:\\Users\\ACER\\.cache\\huggingface\\hub\\models--BAAI--bge-m3\\snapshots\\5617a9f61b028005a4858fdac845db406aefb181\n",
      "04/09/2025 19:03:57 - INFO - BGE_M3.modeling -   loading existing colbert_linear and sparse_linear---------\n",
      "04/09/2025 19:03:57 - INFO - root -   Freeze the parameters for model.embeddings.position_embeddings.weight\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, use_fast=False)\n",
    "\n",
    "model = BGEM3Model(model_name=model_args.model_name_or_path,\n",
    "                   tokenizer=tokenizer,\n",
    "                   normlized=training_args.normlized,\n",
    "                   sentence_pooling_method=training_args.sentence_pooling_method,\n",
    "                   negatives_cross_device=training_args.negatives_cross_device,\n",
    "                   temperature=training_args.temperature,\n",
    "                   enable_sub_batch=training_args.enable_sub_batch,\n",
    "                   unified_finetuning=training_args.unified_finetuning,\n",
    "                   use_self_distill=training_args.use_self_distill,\n",
    "                   colbert_dim=training_args.colbert_dim,\n",
    "                   self_distill_start_step=training_args.self_distill_start_step)\n",
    "\n",
    "if training_args.fix_position_embedding:\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"position_embeddings\" in k:\n",
    "            logging.info(f\"Freeze the parameters for {k}\")\n",
    "            v.requires_grad = False\n",
    "if training_args.fix_encoder:\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"colbert_linear\" in k or 'sparse_linear' in k:\n",
    "            logging.info(f\"train the parameters for {k}\")\n",
    "        else:\n",
    "            v.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTrainDataset(args=data_args, \n",
    "                                        default_batch_size=training_args.per_device_train_batch_size, \n",
    "                                        seed=training_args.seed)\n",
    "data_collator = CustomEmbedCollator(\n",
    "    tokenizer,\n",
    "    query_max_len=data_args.query_max_len,\n",
    "    passage_max_len=data_args.passage_max_len,\n",
    "    sub_batch_size=training_args.sub_batch_size,\n",
    "    pad_to_multiple_of=data_args.pad_to_multiple_of,\n",
    "    padding='max_length',\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_args.per_device_train_batch_size,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số tham số trong mô hình: 567754752\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.model.parameters())\n",
    "\n",
    "print(f\"Tổng số tham số trong mô hình: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embeddings.word_embeddings.weight False\n",
      "model.embeddings.position_embeddings.weight False\n",
      "model.embeddings.token_type_embeddings.weight False\n",
      "model.embeddings.LayerNorm.weight False\n",
      "model.embeddings.LayerNorm.bias False\n",
      "model.encoder.layer.0.attention.self.query.weight False\n",
      "model.encoder.layer.0.attention.self.query.bias False\n",
      "model.encoder.layer.0.attention.self.key.weight False\n",
      "model.encoder.layer.0.attention.self.key.bias False\n",
      "model.encoder.layer.0.attention.self.value.weight False\n",
      "model.encoder.layer.0.attention.self.value.bias False\n",
      "model.encoder.layer.0.attention.output.dense.weight False\n",
      "model.encoder.layer.0.attention.output.dense.bias False\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.0.intermediate.dense.weight False\n",
      "model.encoder.layer.0.intermediate.dense.bias False\n",
      "model.encoder.layer.0.output.dense.weight False\n",
      "model.encoder.layer.0.output.dense.bias False\n",
      "model.encoder.layer.0.output.LayerNorm.weight False\n",
      "model.encoder.layer.0.output.LayerNorm.bias False\n",
      "model.encoder.layer.1.attention.self.query.weight False\n",
      "model.encoder.layer.1.attention.self.query.bias False\n",
      "model.encoder.layer.1.attention.self.key.weight False\n",
      "model.encoder.layer.1.attention.self.key.bias False\n",
      "model.encoder.layer.1.attention.self.value.weight False\n",
      "model.encoder.layer.1.attention.self.value.bias False\n",
      "model.encoder.layer.1.attention.output.dense.weight False\n",
      "model.encoder.layer.1.attention.output.dense.bias False\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.1.intermediate.dense.weight False\n",
      "model.encoder.layer.1.intermediate.dense.bias False\n",
      "model.encoder.layer.1.output.dense.weight False\n",
      "model.encoder.layer.1.output.dense.bias False\n",
      "model.encoder.layer.1.output.LayerNorm.weight False\n",
      "model.encoder.layer.1.output.LayerNorm.bias False\n",
      "model.encoder.layer.2.attention.self.query.weight False\n",
      "model.encoder.layer.2.attention.self.query.bias False\n",
      "model.encoder.layer.2.attention.self.key.weight False\n",
      "model.encoder.layer.2.attention.self.key.bias False\n",
      "model.encoder.layer.2.attention.self.value.weight False\n",
      "model.encoder.layer.2.attention.self.value.bias False\n",
      "model.encoder.layer.2.attention.output.dense.weight False\n",
      "model.encoder.layer.2.attention.output.dense.bias False\n",
      "model.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.2.intermediate.dense.weight False\n",
      "model.encoder.layer.2.intermediate.dense.bias False\n",
      "model.encoder.layer.2.output.dense.weight False\n",
      "model.encoder.layer.2.output.dense.bias False\n",
      "model.encoder.layer.2.output.LayerNorm.weight False\n",
      "model.encoder.layer.2.output.LayerNorm.bias False\n",
      "model.encoder.layer.3.attention.self.query.weight False\n",
      "model.encoder.layer.3.attention.self.query.bias False\n",
      "model.encoder.layer.3.attention.self.key.weight False\n",
      "model.encoder.layer.3.attention.self.key.bias False\n",
      "model.encoder.layer.3.attention.self.value.weight False\n",
      "model.encoder.layer.3.attention.self.value.bias False\n",
      "model.encoder.layer.3.attention.output.dense.weight False\n",
      "model.encoder.layer.3.attention.output.dense.bias False\n",
      "model.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.3.intermediate.dense.weight False\n",
      "model.encoder.layer.3.intermediate.dense.bias False\n",
      "model.encoder.layer.3.output.dense.weight False\n",
      "model.encoder.layer.3.output.dense.bias False\n",
      "model.encoder.layer.3.output.LayerNorm.weight False\n",
      "model.encoder.layer.3.output.LayerNorm.bias False\n",
      "model.encoder.layer.4.attention.self.query.weight False\n",
      "model.encoder.layer.4.attention.self.query.bias False\n",
      "model.encoder.layer.4.attention.self.key.weight False\n",
      "model.encoder.layer.4.attention.self.key.bias False\n",
      "model.encoder.layer.4.attention.self.value.weight False\n",
      "model.encoder.layer.4.attention.self.value.bias False\n",
      "model.encoder.layer.4.attention.output.dense.weight False\n",
      "model.encoder.layer.4.attention.output.dense.bias False\n",
      "model.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.4.intermediate.dense.weight False\n",
      "model.encoder.layer.4.intermediate.dense.bias False\n",
      "model.encoder.layer.4.output.dense.weight False\n",
      "model.encoder.layer.4.output.dense.bias False\n",
      "model.encoder.layer.4.output.LayerNorm.weight False\n",
      "model.encoder.layer.4.output.LayerNorm.bias False\n",
      "model.encoder.layer.5.attention.self.query.weight False\n",
      "model.encoder.layer.5.attention.self.query.bias False\n",
      "model.encoder.layer.5.attention.self.key.weight False\n",
      "model.encoder.layer.5.attention.self.key.bias False\n",
      "model.encoder.layer.5.attention.self.value.weight False\n",
      "model.encoder.layer.5.attention.self.value.bias False\n",
      "model.encoder.layer.5.attention.output.dense.weight False\n",
      "model.encoder.layer.5.attention.output.dense.bias False\n",
      "model.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.5.intermediate.dense.weight False\n",
      "model.encoder.layer.5.intermediate.dense.bias False\n",
      "model.encoder.layer.5.output.dense.weight False\n",
      "model.encoder.layer.5.output.dense.bias False\n",
      "model.encoder.layer.5.output.LayerNorm.weight False\n",
      "model.encoder.layer.5.output.LayerNorm.bias False\n",
      "model.encoder.layer.6.attention.self.query.weight False\n",
      "model.encoder.layer.6.attention.self.query.bias False\n",
      "model.encoder.layer.6.attention.self.key.weight False\n",
      "model.encoder.layer.6.attention.self.key.bias False\n",
      "model.encoder.layer.6.attention.self.value.weight False\n",
      "model.encoder.layer.6.attention.self.value.bias False\n",
      "model.encoder.layer.6.attention.output.dense.weight False\n",
      "model.encoder.layer.6.attention.output.dense.bias False\n",
      "model.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.6.intermediate.dense.weight False\n",
      "model.encoder.layer.6.intermediate.dense.bias False\n",
      "model.encoder.layer.6.output.dense.weight False\n",
      "model.encoder.layer.6.output.dense.bias False\n",
      "model.encoder.layer.6.output.LayerNorm.weight False\n",
      "model.encoder.layer.6.output.LayerNorm.bias False\n",
      "model.encoder.layer.7.attention.self.query.weight False\n",
      "model.encoder.layer.7.attention.self.query.bias False\n",
      "model.encoder.layer.7.attention.self.key.weight False\n",
      "model.encoder.layer.7.attention.self.key.bias False\n",
      "model.encoder.layer.7.attention.self.value.weight False\n",
      "model.encoder.layer.7.attention.self.value.bias False\n",
      "model.encoder.layer.7.attention.output.dense.weight False\n",
      "model.encoder.layer.7.attention.output.dense.bias False\n",
      "model.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.7.intermediate.dense.weight False\n",
      "model.encoder.layer.7.intermediate.dense.bias False\n",
      "model.encoder.layer.7.output.dense.weight False\n",
      "model.encoder.layer.7.output.dense.bias False\n",
      "model.encoder.layer.7.output.LayerNorm.weight False\n",
      "model.encoder.layer.7.output.LayerNorm.bias False\n",
      "model.encoder.layer.8.attention.self.query.weight False\n",
      "model.encoder.layer.8.attention.self.query.bias False\n",
      "model.encoder.layer.8.attention.self.key.weight False\n",
      "model.encoder.layer.8.attention.self.key.bias False\n",
      "model.encoder.layer.8.attention.self.value.weight False\n",
      "model.encoder.layer.8.attention.self.value.bias False\n",
      "model.encoder.layer.8.attention.output.dense.weight False\n",
      "model.encoder.layer.8.attention.output.dense.bias False\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.8.intermediate.dense.weight False\n",
      "model.encoder.layer.8.intermediate.dense.bias False\n",
      "model.encoder.layer.8.output.dense.weight False\n",
      "model.encoder.layer.8.output.dense.bias False\n",
      "model.encoder.layer.8.output.LayerNorm.weight False\n",
      "model.encoder.layer.8.output.LayerNorm.bias False\n",
      "model.encoder.layer.9.attention.self.query.weight False\n",
      "model.encoder.layer.9.attention.self.query.bias False\n",
      "model.encoder.layer.9.attention.self.key.weight False\n",
      "model.encoder.layer.9.attention.self.key.bias False\n",
      "model.encoder.layer.9.attention.self.value.weight False\n",
      "model.encoder.layer.9.attention.self.value.bias False\n",
      "model.encoder.layer.9.attention.output.dense.weight False\n",
      "model.encoder.layer.9.attention.output.dense.bias False\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.9.intermediate.dense.weight False\n",
      "model.encoder.layer.9.intermediate.dense.bias False\n",
      "model.encoder.layer.9.output.dense.weight False\n",
      "model.encoder.layer.9.output.dense.bias False\n",
      "model.encoder.layer.9.output.LayerNorm.weight False\n",
      "model.encoder.layer.9.output.LayerNorm.bias False\n",
      "model.encoder.layer.10.attention.self.query.weight False\n",
      "model.encoder.layer.10.attention.self.query.bias False\n",
      "model.encoder.layer.10.attention.self.key.weight False\n",
      "model.encoder.layer.10.attention.self.key.bias False\n",
      "model.encoder.layer.10.attention.self.value.weight False\n",
      "model.encoder.layer.10.attention.self.value.bias False\n",
      "model.encoder.layer.10.attention.output.dense.weight False\n",
      "model.encoder.layer.10.attention.output.dense.bias False\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.10.intermediate.dense.weight False\n",
      "model.encoder.layer.10.intermediate.dense.bias False\n",
      "model.encoder.layer.10.output.dense.weight False\n",
      "model.encoder.layer.10.output.dense.bias False\n",
      "model.encoder.layer.10.output.LayerNorm.weight False\n",
      "model.encoder.layer.10.output.LayerNorm.bias False\n",
      "model.encoder.layer.11.attention.self.query.weight False\n",
      "model.encoder.layer.11.attention.self.query.bias False\n",
      "model.encoder.layer.11.attention.self.key.weight False\n",
      "model.encoder.layer.11.attention.self.key.bias False\n",
      "model.encoder.layer.11.attention.self.value.weight False\n",
      "model.encoder.layer.11.attention.self.value.bias False\n",
      "model.encoder.layer.11.attention.output.dense.weight False\n",
      "model.encoder.layer.11.attention.output.dense.bias False\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.11.intermediate.dense.weight False\n",
      "model.encoder.layer.11.intermediate.dense.bias False\n",
      "model.encoder.layer.11.output.dense.weight False\n",
      "model.encoder.layer.11.output.dense.bias False\n",
      "model.encoder.layer.11.output.LayerNorm.weight False\n",
      "model.encoder.layer.11.output.LayerNorm.bias False\n",
      "model.encoder.layer.12.attention.self.query.weight False\n",
      "model.encoder.layer.12.attention.self.query.bias False\n",
      "model.encoder.layer.12.attention.self.key.weight False\n",
      "model.encoder.layer.12.attention.self.key.bias False\n",
      "model.encoder.layer.12.attention.self.value.weight False\n",
      "model.encoder.layer.12.attention.self.value.bias False\n",
      "model.encoder.layer.12.attention.output.dense.weight False\n",
      "model.encoder.layer.12.attention.output.dense.bias False\n",
      "model.encoder.layer.12.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.12.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.12.intermediate.dense.weight False\n",
      "model.encoder.layer.12.intermediate.dense.bias False\n",
      "model.encoder.layer.12.output.dense.weight False\n",
      "model.encoder.layer.12.output.dense.bias False\n",
      "model.encoder.layer.12.output.LayerNorm.weight False\n",
      "model.encoder.layer.12.output.LayerNorm.bias False\n",
      "model.encoder.layer.13.attention.self.query.weight False\n",
      "model.encoder.layer.13.attention.self.query.bias False\n",
      "model.encoder.layer.13.attention.self.key.weight False\n",
      "model.encoder.layer.13.attention.self.key.bias False\n",
      "model.encoder.layer.13.attention.self.value.weight False\n",
      "model.encoder.layer.13.attention.self.value.bias False\n",
      "model.encoder.layer.13.attention.output.dense.weight False\n",
      "model.encoder.layer.13.attention.output.dense.bias False\n",
      "model.encoder.layer.13.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.13.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.13.intermediate.dense.weight False\n",
      "model.encoder.layer.13.intermediate.dense.bias False\n",
      "model.encoder.layer.13.output.dense.weight False\n",
      "model.encoder.layer.13.output.dense.bias False\n",
      "model.encoder.layer.13.output.LayerNorm.weight False\n",
      "model.encoder.layer.13.output.LayerNorm.bias False\n",
      "model.encoder.layer.14.attention.self.query.weight False\n",
      "model.encoder.layer.14.attention.self.query.bias False\n",
      "model.encoder.layer.14.attention.self.key.weight False\n",
      "model.encoder.layer.14.attention.self.key.bias False\n",
      "model.encoder.layer.14.attention.self.value.weight False\n",
      "model.encoder.layer.14.attention.self.value.bias False\n",
      "model.encoder.layer.14.attention.output.dense.weight False\n",
      "model.encoder.layer.14.attention.output.dense.bias False\n",
      "model.encoder.layer.14.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.14.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.14.intermediate.dense.weight False\n",
      "model.encoder.layer.14.intermediate.dense.bias False\n",
      "model.encoder.layer.14.output.dense.weight False\n",
      "model.encoder.layer.14.output.dense.bias False\n",
      "model.encoder.layer.14.output.LayerNorm.weight False\n",
      "model.encoder.layer.14.output.LayerNorm.bias False\n",
      "model.encoder.layer.15.attention.self.query.weight False\n",
      "model.encoder.layer.15.attention.self.query.bias False\n",
      "model.encoder.layer.15.attention.self.key.weight False\n",
      "model.encoder.layer.15.attention.self.key.bias False\n",
      "model.encoder.layer.15.attention.self.value.weight False\n",
      "model.encoder.layer.15.attention.self.value.bias False\n",
      "model.encoder.layer.15.attention.output.dense.weight False\n",
      "model.encoder.layer.15.attention.output.dense.bias False\n",
      "model.encoder.layer.15.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.15.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.15.intermediate.dense.weight False\n",
      "model.encoder.layer.15.intermediate.dense.bias False\n",
      "model.encoder.layer.15.output.dense.weight False\n",
      "model.encoder.layer.15.output.dense.bias False\n",
      "model.encoder.layer.15.output.LayerNorm.weight False\n",
      "model.encoder.layer.15.output.LayerNorm.bias False\n",
      "model.encoder.layer.16.attention.self.query.weight False\n",
      "model.encoder.layer.16.attention.self.query.bias False\n",
      "model.encoder.layer.16.attention.self.key.weight False\n",
      "model.encoder.layer.16.attention.self.key.bias False\n",
      "model.encoder.layer.16.attention.self.value.weight False\n",
      "model.encoder.layer.16.attention.self.value.bias False\n",
      "model.encoder.layer.16.attention.output.dense.weight False\n",
      "model.encoder.layer.16.attention.output.dense.bias False\n",
      "model.encoder.layer.16.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.16.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.16.intermediate.dense.weight False\n",
      "model.encoder.layer.16.intermediate.dense.bias False\n",
      "model.encoder.layer.16.output.dense.weight False\n",
      "model.encoder.layer.16.output.dense.bias False\n",
      "model.encoder.layer.16.output.LayerNorm.weight False\n",
      "model.encoder.layer.16.output.LayerNorm.bias False\n",
      "model.encoder.layer.17.attention.self.query.weight False\n",
      "model.encoder.layer.17.attention.self.query.bias False\n",
      "model.encoder.layer.17.attention.self.key.weight False\n",
      "model.encoder.layer.17.attention.self.key.bias False\n",
      "model.encoder.layer.17.attention.self.value.weight False\n",
      "model.encoder.layer.17.attention.self.value.bias False\n",
      "model.encoder.layer.17.attention.output.dense.weight False\n",
      "model.encoder.layer.17.attention.output.dense.bias False\n",
      "model.encoder.layer.17.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.17.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.17.intermediate.dense.weight False\n",
      "model.encoder.layer.17.intermediate.dense.bias False\n",
      "model.encoder.layer.17.output.dense.weight False\n",
      "model.encoder.layer.17.output.dense.bias False\n",
      "model.encoder.layer.17.output.LayerNorm.weight False\n",
      "model.encoder.layer.17.output.LayerNorm.bias False\n",
      "model.encoder.layer.18.attention.self.query.weight False\n",
      "model.encoder.layer.18.attention.self.query.bias False\n",
      "model.encoder.layer.18.attention.self.key.weight False\n",
      "model.encoder.layer.18.attention.self.key.bias False\n",
      "model.encoder.layer.18.attention.self.value.weight False\n",
      "model.encoder.layer.18.attention.self.value.bias False\n",
      "model.encoder.layer.18.attention.output.dense.weight False\n",
      "model.encoder.layer.18.attention.output.dense.bias False\n",
      "model.encoder.layer.18.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.18.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.18.intermediate.dense.weight False\n",
      "model.encoder.layer.18.intermediate.dense.bias False\n",
      "model.encoder.layer.18.output.dense.weight False\n",
      "model.encoder.layer.18.output.dense.bias False\n",
      "model.encoder.layer.18.output.LayerNorm.weight False\n",
      "model.encoder.layer.18.output.LayerNorm.bias False\n",
      "model.encoder.layer.19.attention.self.query.weight False\n",
      "model.encoder.layer.19.attention.self.query.bias False\n",
      "model.encoder.layer.19.attention.self.key.weight False\n",
      "model.encoder.layer.19.attention.self.key.bias False\n",
      "model.encoder.layer.19.attention.self.value.weight False\n",
      "model.encoder.layer.19.attention.self.value.bias False\n",
      "model.encoder.layer.19.attention.output.dense.weight False\n",
      "model.encoder.layer.19.attention.output.dense.bias False\n",
      "model.encoder.layer.19.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.19.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.19.intermediate.dense.weight False\n",
      "model.encoder.layer.19.intermediate.dense.bias False\n",
      "model.encoder.layer.19.output.dense.weight False\n",
      "model.encoder.layer.19.output.dense.bias False\n",
      "model.encoder.layer.19.output.LayerNorm.weight False\n",
      "model.encoder.layer.19.output.LayerNorm.bias False\n",
      "model.encoder.layer.20.attention.self.query.weight False\n",
      "model.encoder.layer.20.attention.self.query.bias False\n",
      "model.encoder.layer.20.attention.self.key.weight False\n",
      "model.encoder.layer.20.attention.self.key.bias False\n",
      "model.encoder.layer.20.attention.self.value.weight False\n",
      "model.encoder.layer.20.attention.self.value.bias False\n",
      "model.encoder.layer.20.attention.output.dense.weight False\n",
      "model.encoder.layer.20.attention.output.dense.bias False\n",
      "model.encoder.layer.20.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.20.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.20.intermediate.dense.weight False\n",
      "model.encoder.layer.20.intermediate.dense.bias False\n",
      "model.encoder.layer.20.output.dense.weight False\n",
      "model.encoder.layer.20.output.dense.bias False\n",
      "model.encoder.layer.20.output.LayerNorm.weight False\n",
      "model.encoder.layer.20.output.LayerNorm.bias False\n",
      "model.encoder.layer.21.attention.self.query.weight False\n",
      "model.encoder.layer.21.attention.self.query.bias False\n",
      "model.encoder.layer.21.attention.self.key.weight False\n",
      "model.encoder.layer.21.attention.self.key.bias False\n",
      "model.encoder.layer.21.attention.self.value.weight False\n",
      "model.encoder.layer.21.attention.self.value.bias False\n",
      "model.encoder.layer.21.attention.output.dense.weight False\n",
      "model.encoder.layer.21.attention.output.dense.bias False\n",
      "model.encoder.layer.21.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.21.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.21.intermediate.dense.weight False\n",
      "model.encoder.layer.21.intermediate.dense.bias False\n",
      "model.encoder.layer.21.output.dense.weight False\n",
      "model.encoder.layer.21.output.dense.bias False\n",
      "model.encoder.layer.21.output.LayerNorm.weight False\n",
      "model.encoder.layer.21.output.LayerNorm.bias False\n",
      "model.encoder.layer.22.attention.self.query.weight False\n",
      "model.encoder.layer.22.attention.self.query.bias False\n",
      "model.encoder.layer.22.attention.self.key.weight False\n",
      "model.encoder.layer.22.attention.self.key.bias False\n",
      "model.encoder.layer.22.attention.self.value.weight False\n",
      "model.encoder.layer.22.attention.self.value.bias False\n",
      "model.encoder.layer.22.attention.output.dense.weight False\n",
      "model.encoder.layer.22.attention.output.dense.bias False\n",
      "model.encoder.layer.22.attention.output.LayerNorm.weight False\n",
      "model.encoder.layer.22.attention.output.LayerNorm.bias False\n",
      "model.encoder.layer.22.intermediate.dense.weight False\n",
      "model.encoder.layer.22.intermediate.dense.bias False\n",
      "model.encoder.layer.22.output.dense.weight False\n",
      "model.encoder.layer.22.output.dense.bias False\n",
      "model.encoder.layer.22.output.LayerNorm.weight False\n",
      "model.encoder.layer.22.output.LayerNorm.bias False\n",
      "model.encoder.layer.23.attention.self.query.weight True\n",
      "model.encoder.layer.23.attention.self.query.bias True\n",
      "model.encoder.layer.23.attention.self.key.weight True\n",
      "model.encoder.layer.23.attention.self.key.bias True\n",
      "model.encoder.layer.23.attention.self.value.weight True\n",
      "model.encoder.layer.23.attention.self.value.bias True\n",
      "model.encoder.layer.23.attention.output.dense.weight True\n",
      "model.encoder.layer.23.attention.output.dense.bias True\n",
      "model.encoder.layer.23.attention.output.LayerNorm.weight True\n",
      "model.encoder.layer.23.attention.output.LayerNorm.bias True\n",
      "model.encoder.layer.23.intermediate.dense.weight True\n",
      "model.encoder.layer.23.intermediate.dense.bias True\n",
      "model.encoder.layer.23.output.dense.weight True\n",
      "model.encoder.layer.23.output.dense.bias True\n",
      "model.encoder.layer.23.output.LayerNorm.weight True\n",
      "model.encoder.layer.23.output.LayerNorm.bias True\n",
      "model.pooler.dense.weight True\n",
      "model.pooler.dense.bias True\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.named_parameters():\n",
    "    if \"embeddings\" in k:\n",
    "        v.requires_grad = False\n",
    "\n",
    "for layer in model.model.encoder.layer[:-1]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for k, v in model.named_parameters():\n",
    "    print(k, v.requires_grad)  # In ra trạng thái của từng tham số trong mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BGEM3Model(\n",
       "  (model): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): XLMRobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (colbert_linear): None\n",
       "  (sparse_linear): None\n",
       "  (cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import BatchEncoding\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_steps = len(train_dataloader)\n",
    "total_traning_steps = num_steps * training_args.num_train_epochs\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(total_traning_steps * training_args.warmup_ratio),\n",
    "    num_training_steps=total_traning_steps,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    print(('\\n' + '%15s' * 3) % ('epoch', 'memory', 'loss'))\n",
    "    p_bar = tqdm(train_dataloader, total=num_steps)\n",
    "    loss_total = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch in p_bar:\n",
    "        batch = {k: v.to(device) if isinstance(v, BatchEncoding) else v for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # outputs = model(**batch)\n",
    "        # loss = outputs.loss\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        loss_total += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        memory = f'{torch.cuda.memory_reserved() / 1E9:.4g}G'  # (GB)\n",
    "        s = ('%15s' * 2 + '%15.5g') % (f'{epoch + 1}/{training_args.num_train_epochs}', memory, loss_total / step)\n",
    "        p_bar.set_description(s)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss_total / step}\")\n",
    "\n",
    "\n",
    "    model.save(training_args.output_dir)\n",
    "    with open(os.path.join(training_args.output_dir, 'loss.txt'), 'a') as f:\n",
    "        f.write(f\"Epoch {epoch + 1}, Loss: {loss_total / step}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BGE_M3.modeling import BGEM3ForInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3', use_fast=False)\n",
    "\n",
    "model_inference = BGEM3ForInference(model_name='./bge-m3-output', tokenizer=tokenizer,\n",
    "                                    enable_sub_batch=False, unified_finetuning=False)\n",
    "\n",
    "model_inference = model_inference.half().to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([2000, 256])\n",
      "Positives shape: torch.Size([2000, 400])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 8\n",
    "queries = []\n",
    "positives = []\n",
    "\n",
    "for item in test_data:\n",
    "    queries.append(item['query'])\n",
    "    positives.append(item['pos'])\n",
    "\n",
    "# encode queries and positives\n",
    "queries = tokenizer.batch_encode_plus(queries, padding='max_length', truncation=True, \n",
    "                                      max_length=256, return_tensors=\"pt\")\n",
    "positives = tokenizer.batch_encode_plus(positives, padding='max_length', truncation=True, \n",
    "                                        max_length=400, return_tensors=\"pt\")\n",
    "\n",
    "dataset = TensorDataset(queries['input_ids'], queries['attention_mask'],\n",
    "                        positives['input_ids'], positives['attention_mask'])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(\"Queries shape:\", queries['input_ids'].shape)\n",
    "print(\"Positives shape:\", positives['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [10:58<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Biến để lưu trữ tất cả các dự đoán\n",
    "queries_dense_vecs = []\n",
    "positives_dense_vecs = []\n",
    "\n",
    "# Duyệt qua DataLoader và đánh giá mô hình\n",
    "with torch.no_grad():  # Không tính toán gradient trong chế độ đánh giá\n",
    "    for batch in tqdm(dataloader):\n",
    "        query_input, query_mask, pos_input, pos_mask = batch\n",
    "        queries = {'input_ids': query_input.to(device), 'attention_mask': query_mask.to(device)}\n",
    "        positives = {'input_ids': pos_input.to(device), 'attention_mask': pos_mask.to(device)}\n",
    "\n",
    "        query_outputs = model_inference(queries)['dense_vecs']\n",
    "        pos_outputs = model_inference(positives)['dense_vecs']\n",
    "        \n",
    "        queries_dense_vecs.append(query_outputs.cpu())\n",
    "        positives_dense_vecs.append(pos_outputs.cpu())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dense_vecs_all = torch.cat(queries_dense_vecs, dim=0)\n",
    "positives_dense_vecs_all = torch.cat(positives_dense_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 2000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = queries_dense_vecs_all @ positives_dense_vecs_all.T\n",
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(similarity, k):\n",
    "    # similarity: ma trận tương đồng giữa các truy vấn và các đoạn văn\n",
    "    # k: số lượng đoạn văn hàng đầu để xem xét\n",
    "    top_k_indices = similarity.topk(k, dim=1).indices\n",
    "    correct_count = 0\n",
    "\n",
    "    for i in range(similarity.size(0)):\n",
    "        if i in top_k_indices[i]:\n",
    "            correct_count += 1\n",
    "\n",
    "    return correct_count / similarity.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.7930\n",
      "Top-3 accuracy: 0.9200\n",
      "Top-5 accuracy: 0.9420\n",
      "Top-10 accuracy: 0.9670\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10]:\n",
    "    acc = top_k_accuracy(similarity, k)\n",
    "    print(f\"Top-{k} accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
