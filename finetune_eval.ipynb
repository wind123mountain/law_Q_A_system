{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:14:34.183457Z",
     "iopub.status.busy": "2025-04-22T10:14:34.183247Z",
     "iopub.status.idle": "2025-04-22T10:14:47.905244Z",
     "shell.execute_reply": "2025-04-22T10:14:47.904648Z",
     "shell.execute_reply.started": "2025-04-22T10:14:34.183439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BGE-M3**\n",
    "\n",
    "**Note:** **[BGE-M3 Fine-tune Guide](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fine-tune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from BGE_M3.arguments import DataArguments, ModelArguments, RetrieverTrainingArguments\n",
    "from BGE_M3.data import CustomTrainDataset, CustomEmbedCollator\n",
    "from BGE_M3.modeling import BGEM3Model\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger('BGE_M3_FINE_TUNE')\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Khởi tạo đối tượng ModelArguments với tham số\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"bge-m3-output\",\n",
    "    tokenizer_name=\"BAAI/bge-m3\",\n",
    "    cache_dir=None\n",
    ")\n",
    "\n",
    "# Khởi tạo đối tượng DataArguments với tham số\n",
    "data_args = DataArguments(\n",
    "    knowledge_distillation=True,\n",
    "    train_data=[\"Data/train_finetune_hn_v3.jsonl\"],\n",
    "    cache_path=\"/data_cache\",\n",
    "    train_group_size=8,\n",
    "    query_max_len=160,\n",
    "    passage_max_len=256,\n",
    "    max_example_num_per_dataset=100000,\n",
    "    same_task_within_batch=True,\n",
    "    shuffle_ratio=0.0,\n",
    "    small_threshold=0,\n",
    "    drop_threshold=0,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Khởi tạo đối tượng RetrieverTrainingArguments với tham số\n",
    "training_args = RetrieverTrainingArguments(\n",
    "    output_dir=\"/kaggle/working/bge-m3-output\",\n",
    "    negatives_cross_device=False,\n",
    "    temperature=0.02,\n",
    "    fix_position_embedding=True,\n",
    "    sentence_pooling_method='cls',\n",
    "    normlized=True,\n",
    "    enable_sub_batch=False,\n",
    "    unified_finetuning=False,\n",
    "    use_self_distill=False,\n",
    "    fix_encoder=False,\n",
    "    colbert_dim=-1,\n",
    "    self_distill_start_step=0,\n",
    "    per_device_train_batch_size=8,\n",
    "    sub_batch_size=-1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, use_fast=False)\n",
    "\n",
    "model = BGEM3Model(model_name=model_args.model_name_or_path,\n",
    "                   tokenizer=tokenizer,\n",
    "                   normlized=training_args.normlized,\n",
    "                   sentence_pooling_method=training_args.sentence_pooling_method,\n",
    "                   negatives_cross_device=training_args.negatives_cross_device,\n",
    "                   temperature=training_args.temperature,\n",
    "                   enable_sub_batch=training_args.enable_sub_batch,\n",
    "                   unified_finetuning=training_args.unified_finetuning,\n",
    "                   use_self_distill=training_args.use_self_distill,\n",
    "                   colbert_dim=training_args.colbert_dim,\n",
    "                   self_distill_start_step=training_args.self_distill_start_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FINETUNE_ALL = False\n",
    "\n",
    "if not FINETUNE_ALL:\n",
    "    if training_args.fix_position_embedding:\n",
    "        for k, v in model.named_parameters():\n",
    "            if \"position_embeddings\" in k:\n",
    "                v.requires_grad = False\n",
    "    if training_args.fix_encoder:\n",
    "        for k, v in model.named_parameters():\n",
    "            if \"colbert_linear\" in k or 'sparse_linear' in k:\n",
    "                v.requires_grad = False\n",
    "    \n",
    "    for k, v in model.named_parameters():\n",
    "        if \"embeddings\" in k:\n",
    "            v.requires_grad = False\n",
    "    \n",
    "    for layer in model.model.encoder.layer[:-12]:\n",
    "        for name, param in layer.named_parameters():\n",
    "            param.requires_grad = False\n",
    "else:\n",
    "    if training_args.fix_position_embedding:\n",
    "        for k, v in model.named_parameters():\n",
    "            if \"position_embeddings\" in k:\n",
    "                v.requires_grad = False\n",
    "    if training_args.fix_encoder:\n",
    "        for k, v in model.named_parameters():\n",
    "            if \"colbert_linear\" in k or 'sparse_linear' in k:\n",
    "                v.requires_grad = False\n",
    "    \n",
    "    for k, v in model.named_parameters():\n",
    "        if \"embeddings\" in k:\n",
    "            v.requires_grad = False\n",
    "\n",
    "# for k, v in model.named_parameters():\n",
    "#     print(k, v.requires_grad)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomTrainDataset(args=data_args, \n",
    "                                        default_batch_size=training_args.per_device_train_batch_size, \n",
    "                                        seed=training_args.seed)\n",
    "data_collator = CustomEmbedCollator(\n",
    "    tokenizer,\n",
    "    query_max_len=data_args.query_max_len,\n",
    "    passage_max_len=data_args.passage_max_len,\n",
    "    sub_batch_size=training_args.sub_batch_size,\n",
    "    pad_to_multiple_of=2,\n",
    "    padding='max_length',\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import BatchEncoding\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "model = model.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_steps = len(train_dataloader)\n",
    "total_traning_steps = num_steps * training_args.num_train_epochs\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "train_dataset.refresh_epoch()\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name='cosine_with_min_lr',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(total_traning_steps * training_args.warmup_ratio),\n",
    "    num_training_steps=total_traning_steps,\n",
    "    scheduler_specific_kwargs={'min_lr': 1e-6}\n",
    ")\n",
    "\n",
    "training_args.num_train_epochs = 5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    print(('\\n' + '%15s' * 3) % ('epoch', 'memory', 'loss'))\n",
    "    p_bar = tqdm(train_dataloader, total=num_steps)\n",
    "    loss_total = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch in p_bar:\n",
    "        batch = {k: v.to(device) if isinstance(v, BatchEncoding) else v for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_total += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        memory = f'{torch.cuda.memory_reserved() / 1E9:.4g}G'  # (GB)\n",
    "        s = ('%15s' * 2 + '%15.5g') % (f'{epoch + 1}/{training_args.num_train_epochs}', memory, loss_total / step)\n",
    "        p_bar.set_description(s)\n",
    "\n",
    "    train_dataset.refresh_epoch()\n",
    "\n",
    "    model.save(training_args.output_dir)\n",
    "    with open(os.path.join(training_args.output_dir, 'loss.txt'), 'a') as f:\n",
    "        f.write(f\"Epoch {epoch + 1}, Loss: {loss_total / step}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from BGE_M3.utils import BGEM3Eval\n",
    "\n",
    "m3_eval = BGEM3Eval(model_name=\"BAAI/bge-m3\",\n",
    "                    tokenizer_name='BAAI/bge-m3',\n",
    "                    data_path='Data/test_finetune_hn.jsonl',\n",
    "                    candidate_pool='Data/corpus.jsonl',\n",
    "                    batch_size=128, query_max_length=160, positive_max_length=256, max_neg=100)\n",
    "\n",
    "queries_dense_vecs, positives_dense_vecs = m3_eval.inference()\n",
    "\n",
    "for k in [1, 3, 5, 10, 100]:\n",
    "    acc, recall, _ = BGEM3Eval.top_k_accuracy_recall(queries_dense_vecs,\n",
    "                                                     positives_dense_vecs,\n",
    "                                                     m3_eval.labels, k)\n",
    "    print(f\"\\nTop-{k} accuracy: {acc:.4f}\", \"---\", f\"Top-{k} reacll: {recall:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from BGE_M3.utils import BGEM3Eval\n",
    "\n",
    "m3_eval = BGEM3Eval(model_name=\"bge-m3-output\",\n",
    "                    tokenizer_name='BAAI/bge-m3',\n",
    "                    data_path='Data/test_finetune_hn.jsonl',\n",
    "                    candidate_pool='Data/corpus.jsonl',\n",
    "                    batch_size=128, query_max_length=160, positive_max_length=256, max_neg=100)\n",
    "\n",
    "queries_dense_vecs, positives_dense_vecs = m3_eval.inference()\n",
    "\n",
    "for k in [1, 3, 5, 10, 100]:\n",
    "    acc, recall, _ = BGEM3Eval.top_k_accuracy_recall(queries_dense_vecs,\n",
    "                                                     positives_dense_vecs,\n",
    "                                                     m3_eval.labels, k)\n",
    "    print(f\"\\nTop-{k} accuracy: {acc:.4f}\", \"---\", f\"Top-{k} reacll: {recall:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BGE-M3-Reanker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Finetune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bge-m3-reranker-output\")\n",
    "\n",
    "\n",
    "FINETUNE_ALL = True\n",
    "USE_LORA = False\n",
    "\n",
    "if not FINETUNE_ALL:\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"position_embeddings\" in k:\n",
    "            v.requires_grad = False\n",
    "\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"embeddings\" in k:\n",
    "            v.requires_grad = False\n",
    "\n",
    "    for layer in model.roberta.encoder.layer[:-15]:\n",
    "        for name, param in layer.named_parameters():\n",
    "            param.requires_grad = False\n",
    "else:\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"position_embeddings\" in k:\n",
    "            v.requires_grad = False\n",
    "\n",
    "    for k, v in model.named_parameters():\n",
    "        if \"embeddings\" in k:\n",
    "            v.requires_grad = False\n",
    "\n",
    "# for k, v in model.named_parameters():\n",
    "#     print(k, v.requires_grad)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T03:21:04.277292Z",
     "iopub.status.busy": "2025-04-18T03:21:04.276445Z",
     "iopub.status.idle": "2025-04-18T03:21:04.334155Z",
     "shell.execute_reply": "2025-04-18T03:21:04.333475Z",
     "shell.execute_reply.started": "2025-04-18T03:21:04.277262Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from BGE_M3.data import CrossDataset, RerankerCollator\n",
    "\n",
    "\n",
    "dataset = CrossDataset(tokenizer, \n",
    "                       'Data/train_finetune_rerank_hn.jsonl',\n",
    "                       160, 256, 3)\n",
    "\n",
    "print(\"samples:\", dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T03:29:14.001168Z",
     "iopub.status.busy": "2025-04-18T03:29:14.000681Z",
     "iopub.status.idle": "2025-04-18T05:05:00.743380Z",
     "shell.execute_reply": "2025-04-18T05:05:00.742589Z",
     "shell.execute_reply.started": "2025-04-18T03:29:14.001147Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          epoch         memory           loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "            1/3          13.4G        0.29176: 100%|██████████| 1706/1706 [32:12<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          epoch         memory           loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "            2/3          13.4G        0.22979: 100%|██████████| 1706/1706 [31:44<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          epoch         memory           loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "            3/3          13.4G         0.1641: 100%|██████████| 1706/1706 [31:29<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import get_scheduler\n",
    "import os\n",
    "\n",
    "output_dir = '/kaggle/working/bge-m3-reranker-output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
    "                        collate_fn=RerankerCollator(tokenizer))\n",
    "num_steps = len(dataloader)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device).train()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name='cosine_with_min_lr',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(num_steps * epochs * 0.1),\n",
    "    num_training_steps=num_steps * epochs,\n",
    "    scheduler_specific_kwargs={'min_lr': 1e-6}\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(('\\n' + '%15s' * 3) % ('epoch', 'memory', 'loss'))\n",
    "    p_bar = tqdm(dataloader, total=num_steps)\n",
    "    loss_total = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch in p_bar:\n",
    "        inpusts = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            logits = model(**inpusts).logits\n",
    "            grouped_logits = logits.view(batch_size, -1)\n",
    "            target = torch.zeros(batch_size, device=device, dtype=torch.long)\n",
    "            \n",
    "            loss = criterion(grouped_logits, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_total += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        memory = f'{torch.cuda.memory_reserved() / 1E9:.4g}G'  # (GB)\n",
    "        s = ('%15s' * 2 + '%15.5g') % (f'{epoch + 1}/{epochs}', memory, loss_total / step)\n",
    "        p_bar.set_description(s)\n",
    "\n",
    "    model.save_pretrained(output_dir, state_dict=model.state_dict())\n",
    "    with open(output_dir + '/loss.txt', 'a') as f:\n",
    "        f.write(f\"Epoch {epoch + 1}, Loss: {loss_total / step}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T08:01:34.031521Z",
     "iopub.status.busy": "2025-04-18T08:01:34.030857Z",
     "iopub.status.idle": "2025-04-18T08:01:34.454174Z",
     "shell.execute_reply": "2025-04-18T08:01:34.453463Z",
     "shell.execute_reply.started": "2025-04-18T08:01:34.031498Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T05:06:03.965906Z",
     "iopub.status.busy": "2025-04-18T05:06:03.965339Z",
     "iopub.status.idle": "2025-04-18T05:06:04.023045Z",
     "shell.execute_reply": "2025-04-18T05:06:04.022484Z",
     "shell.execute_reply.started": "2025-04-18T05:06:03.965885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from BGE_M3.utils import BGEM3Eval, Reranker, reranker_top_k_accuracy_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-04-18T05:07:23.536058Z",
     "shell.execute_reply": "2025-04-18T05:07:23.535423Z",
     "shell.execute_reply.started": "2025-04-18T05:06:10.685723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([2048, 160])\n",
      "Passage shape: torch.Size([31527, 256])\n"
     ]
    }
   ],
   "source": [
    "m3_eval = BGEM3Eval(model_name=\"bge-m3-output\",\n",
    "                    tokenizer_name='BAAI/bge-m3',\n",
    "                    data_path='Data/test_finetune_hn.jsonl',\n",
    "                    candidate_pool='Data/corpus.jsonl',\n",
    "                    batch_size=128, query_max_length=160, positive_max_length=256, max_neg=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T05:07:23.540149Z",
     "iopub.status.busy": "2025-04-18T05:07:23.539877Z",
     "iopub.status.idle": "2025-04-18T05:13:32.471746Z",
     "shell.execute_reply": "2025-04-18T05:13:32.471037Z",
     "shell.execute_reply.started": "2025-04-18T05:07:23.540133Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries inferencing: 100%|██████████| 16/16 [00:14<00:00,  1.08it/s]\n",
      "Passage inferencing: 100%|██████████| 247/247 [05:54<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "queries_dense_vecs, positives_dense_vecs = m3_eval.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T05:13:32.472933Z",
     "iopub.status.busy": "2025-04-18T05:13:32.472681Z",
     "iopub.status.idle": "2025-04-18T05:13:33.256842Z",
     "shell.execute_reply": "2025-04-18T05:13:33.256061Z",
     "shell.execute_reply.started": "2025-04-18T05:13:32.472914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reranker = Reranker('bge-m3-reranker-output',\n",
    "                    'BAAI/bge-reranker-v2-m3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T13:25:36.890223Z",
     "iopub.status.busy": "2025-04-17T13:25:36.889602Z",
     "iopub.status.idle": "2025-04-17T13:33:46.772933Z",
     "shell.execute_reply": "2025-04-17T13:33:46.772301Z",
     "shell.execute_reply.started": "2025-04-17T13:25:36.890193Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [08:08,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing metric...\n",
      "\n",
      "Top-1 accuracy: 0.5713 --- Top-1 reacll: 0.3758\n",
      "\n",
      "\n",
      "Top-3 accuracy: 0.8047 --- Top-3 reacll: 0.6409\n",
      "\n",
      "\n",
      "Top-5 accuracy: 0.8750 --- Top-5 reacll: 0.7259\n",
      "\n",
      "\n",
      "Top-10 accuracy: 0.9199 --- Top-10 reacll: 0.8077\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reranker_top_k_accuracy_recall(reranker, queries_dense_vecs, positives_dense_vecs,\n",
    "                               m3_eval.corpus, m3_eval.queries, m3_eval.labels,\n",
    "                               k_list = [1, 3, 5, 10], n_candidate=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ensemble BM25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:23:33.704184Z",
     "iopub.status.busy": "2025-04-22T11:23:33.703993Z",
     "iopub.status.idle": "2025-04-22T11:23:46.648037Z",
     "shell.execute_reply": "2025-04-22T11:23:46.647483Z",
     "shell.execute_reply.started": "2025-04-22T11:23:33.704170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from BGE_M3.utils import Reranker\n",
    "from BGE_M3.modeling import BGEM3ForInference\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from pyvi import ViTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-04-22T10:15:55.981016Z",
     "shell.execute_reply": "2025-04-22T10:15:55.980400Z",
     "shell.execute_reply.started": "2025-04-22T10:14:48.070468Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-m3', use_fast=False)\n",
    "\n",
    "model_inference = BGEM3ForInference(model_name='bge-m3-output', \n",
    "                                    tokenizer=tokenizer, enable_sub_batch=False, \n",
    "                                    unified_finetuning=False)\n",
    "\n",
    "m3_eval_model = model_inference.to(device).half().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:25:18.567990Z",
     "iopub.status.busy": "2025-04-22T11:25:18.567386Z",
     "iopub.status.idle": "2025-04-22T11:25:19.269840Z",
     "shell.execute_reply": "2025-04-22T11:25:19.268939Z",
     "shell.execute_reply.started": "2025-04-22T11:25:18.567965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('bm25-eval/queries.jsonl', 'r', encoding='utf-8') as f:\n",
    "    queries = [json.loads(line.strip())['text'] for line in f]\n",
    "\n",
    "with open('bm25-eval/corpus.jsonl', 'r', encoding='utf-8') as f:\n",
    "    corpus = [json.loads(line.strip())['text'] for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:19:29.967548Z",
     "iopub.status.busy": "2025-04-22T10:19:29.967025Z",
     "iopub.status.idle": "2025-04-22T10:19:58.332956Z",
     "shell.execute_reply": "2025-04-22T10:19:58.332010Z",
     "shell.execute_reply.started": "2025-04-22T10:19:29.967526Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([2048, 160])\n",
      "Passage shape: torch.Size([31527, 256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "\n",
    "queries_tk = tokenizer.batch_encode_plus(queries, padding='max_length', truncation=True, \n",
    "                                           max_length=160, return_tensors=\"pt\")\n",
    "passage_tk = tokenizer.batch_encode_plus(corpus, padding='max_length', truncation=True,\n",
    "                                             max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "q_dataset = TensorDataset(queries_tk['input_ids'], queries_tk['attention_mask'])\n",
    "p_dataset = TensorDataset(passage_tk['input_ids'], passage_tk['attention_mask'])\n",
    "\n",
    "q_dataloader = DataLoader(q_dataset, batch_size=batch_size)\n",
    "p_dataloader = DataLoader(p_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"Queries shape:\", queries_tk['input_ids'].shape)\n",
    "print(\"Passage shape:\", passage_tk['input_ids'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T10:20:38.421564Z",
     "iopub.status.busy": "2025-04-22T10:20:38.421252Z",
     "iopub.status.idle": "2025-04-22T10:26:53.429222Z",
     "shell.execute_reply": "2025-04-22T10:26:53.428280Z",
     "shell.execute_reply.started": "2025-04-22T10:20:38.421542Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Queries inferencing: 100%|██████████| 8/8 [00:11<00:00,  1.50s/it]\n",
      "Passage inferencing: 100%|██████████| 124/124 [06:02<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "queries_dense_vecs = []\n",
    "passages_dense_vecs = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(q_dataloader, desc=\"Queries inferencing\"):\n",
    "        query_input, query_mask = batch\n",
    "        query_ips = {'input_ids': query_input.to(device),\n",
    "                   'attention_mask': query_mask.to(device)}\n",
    "\n",
    "        query_outputs = m3_eval_model(query_ips)['dense_vecs']\n",
    "        \n",
    "        queries_dense_vecs.append(query_outputs.cpu())\n",
    "\n",
    "    for batch in tqdm(p_dataloader, desc=\"Passage inferencing\"):\n",
    "        p_input, p_mask = batch\n",
    "        passages = {'input_ids': p_input.to(device), \n",
    "                    'attention_mask': p_mask.to(device)}\n",
    "\n",
    "        passages_outputs = m3_eval_model(passages)['dense_vecs']\n",
    "        \n",
    "        passages_dense_vecs.append(passages_outputs.cpu())\n",
    "\n",
    "queries_dense_vecs = torch.cat(queries_dense_vecs, dim=0)\n",
    "passages_dense_vecs = torch.cat(passages_dense_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:23:46.649039Z",
     "iopub.status.busy": "2025-04-22T11:23:46.648692Z",
     "iopub.status.idle": "2025-04-22T11:23:54.219092Z",
     "shell.execute_reply": "2025-04-22T11:23:54.218506Z",
     "shell.execute_reply.started": "2025-04-22T11:23:46.649022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tách từ từng câu trong corpus\n",
    "tokenized_corpus = [ViTokenizer.tokenize(doc.lower()).split() for doc in corpus]\n",
    "\n",
    "# Khởi tạo BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "tokenized_queries = [ViTokenizer.tokenize(doc.lower()).split() for doc in queries]\n",
    "\n",
    "queries_bm25_scores = [bm25.get_scores(tokenized_query) for tokenized_query in tokenized_queries]\n",
    "\n",
    "with open('bm25-eval/queries_bm25_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(queries_bm25_scores, f)\n",
    "\n",
    "\n",
    "# with open('bm25-eval/queries_bm25_scores.pkl', 'rb') as f:\n",
    "#     queries_bm25_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:23:54.220078Z",
     "iopub.status.busy": "2025-04-22T11:23:54.219845Z",
     "iopub.status.idle": "2025-04-22T11:23:54.228835Z",
     "shell.execute_reply": "2025-04-22T11:23:54.228340Z",
     "shell.execute_reply.started": "2025-04-22T11:23:54.220053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('bm25-eval/labels.pkl', 'rb') as f:\n",
    "    lables = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:23:54.229714Z",
     "iopub.status.busy": "2025-04-22T11:23:54.229485Z",
     "iopub.status.idle": "2025-04-22T11:23:55.733299Z",
     "shell.execute_reply": "2025-04-22T11:23:55.732484Z",
     "shell.execute_reply.started": "2025-04-22T11:23:54.229675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sims = queries_dense_vecs @ passages_dense_vecs.T\n",
    "\n",
    "sims = sims.numpy()\n",
    "\n",
    "# with open('bm25-eval/bge_m3_scores.pkl', 'rb') as f:\n",
    "#     sims = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:23:55.736940Z",
     "iopub.status.busy": "2025-04-22T11:23:55.736638Z",
     "iopub.status.idle": "2025-04-22T11:23:56.357802Z",
     "shell.execute_reply": "2025-04-22T11:23:56.356911Z",
     "shell.execute_reply.started": "2025-04-22T11:23:55.736914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "l2_norms = np.linalg.norm(queries_bm25_scores, axis=1, keepdims=True)\n",
    "queries_bm25_scores_normalized = queries_bm25_scores / l2_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:27:46.507556Z",
     "iopub.status.busy": "2025-04-22T11:27:46.506944Z",
     "iopub.status.idle": "2025-04-22T11:27:49.431318Z",
     "shell.execute_reply": "2025-04-22T11:27:49.430744Z",
     "shell.execute_reply.started": "2025-04-22T11:27:46.507536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fusion_scores = sims + 3*np.array(queries_bm25_scores_normalized)\n",
    "\n",
    "fusion_rank = np.argsort(-fusion_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:27:53.185109Z",
     "iopub.status.busy": "2025-04-22T11:27:53.184853Z",
     "iopub.status.idle": "2025-04-22T11:27:53.252475Z",
     "shell.execute_reply": "2025-04-22T11:27:53.251945Z",
     "shell.execute_reply.started": "2025-04-22T11:27:53.185093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing metric...\n",
      "\n",
      "Top-1 accuracy: 0.5469 --- Top-1 reacll: 0.3563\n",
      "\n",
      "\n",
      "Top-3 accuracy: 0.8008 --- Top-3 reacll: 0.6353\n",
      "\n",
      "\n",
      "Top-5 accuracy: 0.8682 --- Top-5 reacll: 0.7181\n",
      "\n",
      "\n",
      "Top-10 accuracy: 0.9219 --- Top-10 reacll: 0.8066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('computing metric...')\n",
    "for k in [1, 3, 5, 10]:\n",
    "  sum_recall = 0\n",
    "  sum_acc = 0\n",
    "\n",
    "  for i, rel_indexs in enumerate(lables):\n",
    "      topk_candidates = fusion_rank[i][:k]\n",
    "      n_recall = 0\n",
    "      for rel_indx in rel_indexs:\n",
    "          if rel_indx in topk_candidates:\n",
    "              n_recall += 1\n",
    "      if n_recall > 0:\n",
    "          sum_acc += 1\n",
    "      sum_recall += n_recall / len(rel_indexs)\n",
    "\n",
    "  acc = sum_acc / len(fusion_rank)\n",
    "  recall = sum_recall / len(fusion_rank)\n",
    "\n",
    "  print(f\"\\nTop-{k} accuracy: {acc:.4f}\", \"---\", f\"Top-{k} reacll: {recall:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reranker = Reranker('bge-m3-reranker-output',\n",
    "                    'BAAI/bge-reranker-v2-m3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:29:42.919905Z",
     "iopub.status.busy": "2025-04-22T11:29:42.919313Z",
     "iopub.status.idle": "2025-04-22T11:39:13.897496Z",
     "shell.execute_reply": "2025-04-22T11:39:13.896907Z",
     "shell.execute_reply.started": "2025-04-22T11:29:42.919879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2048it [09:30,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "print('reranking...')\n",
    "reranking = []\n",
    "for i, query in tqdm(enumerate(queries)):\n",
    "  passages = [corpus[idx] for idx in fusion_rank[i][:16]]\n",
    "  sorted_indices = reranker.rerank(query, passages)\n",
    "  reranking.append([fusion_rank[i][idx] for idx in sorted_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T11:39:13.898791Z",
     "iopub.status.busy": "2025-04-22T11:39:13.898536Z",
     "iopub.status.idle": "2025-04-22T11:39:13.912437Z",
     "shell.execute_reply": "2025-04-22T11:39:13.911807Z",
     "shell.execute_reply.started": "2025-04-22T11:39:13.898772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing metric...\n",
      "\n",
      "Top-1 accuracy: 0.5605 --- Top-1 reacll: 0.3713\n",
      "\n",
      "\n",
      "Top-3 accuracy: 0.8101 --- Top-3 reacll: 0.6506\n",
      "\n",
      "\n",
      "Top-5 accuracy: 0.8735 --- Top-5 reacll: 0.7299\n",
      "\n",
      "\n",
      "Top-10 accuracy: 0.9209 --- Top-10 reacll: 0.8140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('computing metric...')\n",
    "for k in [1, 3, 5, 10]:\n",
    "  sum_recall = 0\n",
    "  sum_acc = 0\n",
    "\n",
    "  for i, rel_indexs in enumerate(lables):\n",
    "      topk_candidates = reranking[i][:k]\n",
    "      n_recall = 0\n",
    "      for rel_indx in rel_indexs:\n",
    "          if rel_indx in topk_candidates:\n",
    "              n_recall += 1\n",
    "      if n_recall > 0:\n",
    "          sum_acc += 1\n",
    "      sum_recall += n_recall / len(rel_indexs)\n",
    "\n",
    "  acc = sum_acc / len(fusion_rank)\n",
    "  recall = sum_recall / len(fusion_rank)\n",
    "\n",
    "  print(f\"\\nTop-{k} accuracy: {acc:.4f}\", \"---\", f\"Top-{k} reacll: {recall:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7124478,
     "sourceId": 11378983,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7172134,
     "sourceId": 11447800,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7123848,
     "sourceId": 11450777,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7172136,
     "sourceId": 11456158,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7124337,
     "sourceId": 11460641,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7189900,
     "sourceId": 11513153,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
